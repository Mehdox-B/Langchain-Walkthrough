{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load you env variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import openai\n",
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "_ = load_dotenv(find_dotenv(filename='.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import  ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "## Specify the buffer memory for storing the chat History\n",
    "memory = ConversationBufferMemory()\n",
    "## Start a conversation chains that take into consideration a buffer memory \n",
    "chatConversation = ConversationChain(\n",
    "    llm=llm_model,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello There assistant , my name is mehdox how are you ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Mehdox! I'm doing well, thank you for asking. How can I assist you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Start a conversation by asking a question to the LLM\n",
    "chatConversation.predict(input=\"Hello There assistant , my name is mehdox how are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello There assistant , my name is mehdox how are you ?\n",
      "AI: Hello Mehdox! I'm doing well, thank you for asking. How can I assist you today?\n",
      "Human: Can you guesse what is my name ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the information you provided, your name is Mehdox.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatConversation.predict(input=\"Can you guesse what is my name ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello There assistant , my name is mehdox how are you ?\\nAI: Hello Mehdox! I'm doing well, thank you for asking. How can I assist you today?\\nHuman: Can you guesse what is my name ?\\nAI: Based on the information you provided, your name is Mehdox.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Conversation Chains Chat History \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import  ConversationBufferWindowMemory\n",
    "## keeping a sliding window of the most recent interactions (k=Window size/input-output), so the buffer does not get too large.\n",
    "bufferWindowMemory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bufferWindowMemory.save_context(\n",
    "    {\n",
    "        \"input\":\"Hello There ChatGPT how are you ?\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"fine thank you how can i assist you today\"\n",
    "    }\n",
    ")\n",
    "bufferWindowMemory.save_context(\n",
    "    {\n",
    "        \"input\": \"Not much, just hanging\"\n",
    "    },\n",
    "    {\n",
    "        \"output\": \"Cool\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hello There ChatGPT how are you ?\\nAI: fine thank you how can i assist you today\\nHuman: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bufferWindowMemory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start a conversation chains that take into consideration a buffer Window memory \n",
    "chatBufferWindowConversation = ConversationChain(\n",
    "    llm=llm_model,\n",
    "    memory=bufferWindowMemory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: Hello there assistant , my name is mehdox remember me ok ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Mehdox! Of course, I remember you. How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBufferWindowConversation.predict(input=\"Hello there assistant , my name is mehdox remember me ok ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello there assistant , my name is mehdox remember me ok ?\n",
      "AI: Hello Mehdox! Of course, I remember you. How can I assist you today?\n",
      "Human: give me the result of this math expression exp(0)\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The result of the math expression exp(0) is 1. The exponential function raised to the power of 0 always equals 1. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBufferWindowConversation.predict(input=\"give me the result of this math expression exp(0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: give me the result of this math expression exp(0)\n",
      "AI: The result of the math expression exp(0) is 1. The exponential function raised to the power of 0 always equals 1. Is there anything else you would like to know?\n",
      "Human: I am mehdox , do you rememer me ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Mehdox! I'm sorry, but I don't have the ability to remember individual users. Is there anything else you would like to know or talk about?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatBufferWindowConversation.predict(input=\"I am mehdox , do you rememer me ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What if the buffer memory is controlled based on the number of tokens instead of windows size\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "tokenBufferMemory = ConversationTokenBufferMemory(\n",
    "    llm=llm_model,\n",
    "    max_token_limit=40,\n",
    ")\n",
    "chatTokenBufferMemory = ConversationChain(\n",
    "    llm=llm_model,\n",
    "    memory=tokenBufferMemory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenBufferMemory.save_context(\n",
    "    {\n",
    "        \"input\":\"Langchain is what ?\"\n",
    "    },\n",
    "    {\n",
    "        \"output\":\"Amazing Framework for LLM apps\"\n",
    "    }\n",
    ")\n",
    "tokenBufferMemory.save_context(\n",
    "     {\n",
    "        \"input\":\"Azure data factory is what ?\"\n",
    "    },\n",
    "    {\n",
    "        \"output\":\"is a ETL Cloud based service provided by azure cloud\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing Framework for LLM apps\\nHuman: Azure data factory is what ?\\nAI: is a ETL Cloud based service provided by azure cloud'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenBufferMemory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "summaryMemory = ConversationSummaryBufferMemory(\n",
    "    llm=llm_model,\n",
    "    max_token_limit=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "summaryMemory.save_context(\n",
    "    {\"input\": \"Hello\"}, \n",
    "    {\"output\": \"What's up\"}\n",
    "    )\n",
    "summaryMemory.save_context(\n",
    "    {\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "summaryMemory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI and asks about the schedule for the day. The AI informs the human about a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's activities.\"}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryMemory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryMemoryChat = ConversationChain(\n",
    "    llm=llm_model,\n",
    "    memory=summaryMemory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks about the schedule for the day. The AI informs the human about a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's activities.\n",
      "Human: What is the main task in this noon ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The main task for this noon is the lunch meeting with a customer interested in AI. It's a great opportunity to showcase our expertise and potentially secure a new partnership. I recommend preparing some key talking points and examples of our successful AI projects to impress the customer.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryMemoryChat.predict(input=\"What is the main task in this noon ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI and asks about the schedule for the day. The AI informs the human about a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's activities.\\nHuman: What is the main task in this noon ?\\nAI: The main task for this noon is the lunch meeting with a customer interested in AI. It's a great opportunity to showcase our expertise and potentially secure a new partnership. I recommend preparing some key talking points and examples of our successful AI projects to impress the customer.\"}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryMemory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tuto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
