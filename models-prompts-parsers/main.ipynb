{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "\n",
    "## Outline :\n",
    "\n",
    " * Direct API calls to OpenAI/Gemini Pro/Lamma3\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load you env variables\n",
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import openai\n",
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "_ = load_dotenv(find_dotenv(filename='.env'))\n",
    "print(\"your openai api key is {OPENAI_API_KEY}\".format(OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")))\n",
    "print(\"your claud api key is {ANTHROPIC_API_KEY}\".format(ANTHROPIC_API_KEY=os.getenv(\"ANTHROPIC_API_KEY\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_call(prompt,model=\"gpt-4o\"):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":prompt\n",
    "        }\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework designed to facilitate the development of applications that leverage large language models (LLMs). It provides a suite of tools and abstractions that make it easier to build complex applications that can interact with LLMs in a meaningful way. LangChain is particularly useful for applications that require:\\n\\n1. **Data Augmented Generation**: Enhancing the capabilities of LLMs by integrating external data sources.\\n2. **Agentic Behavior**: Allowing LLMs to perform actions, interact with their environment, and make decisions based on the context.\\n3. **Memory**: Enabling LLMs to remember past interactions and use this memory to inform future responses.\\n\\nLangChain supports various components to achieve these functionalities:\\n\\n- **Prompt Templates**: Standardized templates for generating prompts that can be used to query LLMs.\\n- **Chains**: Sequences of calls to LLMs or other utilities that can be combined to perform complex tasks.\\n- **Indexes**: Structures for efficiently storing and retrieving information that LLMs can use.\\n- **Agents**: Components that can make decisions and perform actions based on LLM outputs.\\n- **Memory**: Mechanisms for storing and retrieving past interactions to provide context to LLMs.\\n\\nLangChain is designed to be modular and flexible, allowing developers to customize and extend its components to fit their specific needs. It is particularly well-suited for building applications in areas such as natural language understanding, conversational agents, and data-driven content generation.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_call(\"What is Langchain ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design custom prompt basesd on specific user message\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey\n",
    "\"\"\"\n",
    "style = \"\"\"Dialect Darija with respectfull language\"\"\"\n",
    "designed_prompt = f\"\"\"\n",
    "Translate in arabic darija the text that is delimited by triple backricks \\\n",
    "with a style that is {style}.\n",
    "text:```{customer_email}``` \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate in arabic darija the text that is delimited by triple backricks with a style that is Dialect Darija with respectfull language.\n",
      "text:```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey\n",
      "``` \n"
     ]
    }
   ],
   "source": [
    "## get the formated prompt\n",
    "print(designed_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```darija\\nآآآه، راه أنا معصب بزاف حيث غطا ديال البلندر طار ووسخ لي حيطان المطبخ ديالي بالسmoothie! وزيد عليها، الضمانة ما كاتغطيش تكاليف تنظيف المطبخ ديالي. خصني مساعدتك دابا، صاحبي.\\n```'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_call(prompt=designed_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "claud_client = Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claudai_call(prompt,model=\"claude-3-opus-20240229\"):\n",
    "    input_messages=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"Hello claud , can you assist me on this conversation !\",\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":\"of course , let me know about your question\",\n",
    "            \"role\":\"user\",\n",
    "            \"content\":prompt\n",
    "        }\n",
    "    ]\n",
    "    claud_response=claud_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        messages=input_messages,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return claud_response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claudai_call(prompt=designed_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat API : LangChain Framework\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate the OpenAI Langchain Wrapper\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001FD286F6680>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001FD286F7D90>, temperature=0.2, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2,\n",
    "    streaming=True,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "openai_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate me the text that is delimited by triple backticks into arabic .With the following style : {style}\n",
      "text: ```{text}```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Define the custom Template \n",
    "style = \"structured and readable language\"\n",
    "text = \"Hello everyone your are in the langchain tutorial . Happy Learning\"\n",
    "template_string= \"\"\"\n",
    "Translate me the text that is delimited by triple backticks into arabic .\\\n",
    "With the following style : {style}\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "print(template_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='\\nTranslate me the text that is delimited by triple backticks into arabic .With the following style : structured and readable language\\ntext: ```Hello everyone your are in the langchain tutorial . Happy Learning```\\n')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "   style=style,\n",
    "   text=text\n",
    ")\n",
    "customer_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'مرحبًا بالجميع، أنتم الآن في دورة تعليم لغة البرمجة. نتمنى لكم تعلمًا ممتعًا.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Call the LLM for generating the response based on the prompt template\n",
    "customer_response = openai_chat_model.invoke(customer_messages)\n",
    "customer_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## structure of the final output\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"gift\": true,\\n    \"delivery_days\": 2,\\n    \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there\"]\\n}'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_review_template = ChatPromptTemplate.from_template(\n",
    "    review_template\n",
    ")\n",
    "customer_review_messages = customer_review_template.format_messages(text=customer_review)\n",
    "llm__review = openai_chat_model.invoke(customer_review_messages)\n",
    "llm__review.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the generated response\n",
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "llm__review.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Output Parser for more suitable type of responses\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "jsonParser = JsonOutputParser()\n",
    "jsonparsed_llm_review = jsonParser.invoke(llm__review.content)\n",
    "type(jsonparsed_llm_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's slightly more expensive than the other leaf blowers out there\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonparsed_llm_review.get('price_value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tuto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
